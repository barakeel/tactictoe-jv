\documentclass[runningheads,a4paper,draft]{svjour3}

%\usepackage{etex}
%packages indispensables
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{todonotes}
\newcommand{\todoi}[1]{\todo[inline]{#1}}

%\usepackage{graphicx}
%packages utiles
\usepackage{alltt} %program code
%\usepackage{enumerate}
%\usepackage{amssymb} %lettres mathÃ©matiques
%\let\oldemptyset\emptyset
%let\emptyset\varnothing
%\usepackage{amsthm}
\usepackage{amsfonts}
%\usepackage{bussproofs} %derivation
%\usepackage{hyperref} %to write path.
%\usepackage{color} % colouring text
%\usepackage{tabularx} % table
%\usepackage{fancyvrb}
%\usepackage[toc,page]{appendix}
%\usepackage{cite}
\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{pgfplots}
%\usepackage{float}
%\restylefloat{table}
\usepackage{xcolor}
\usepackage{url}
\usepackage{subfigure}

\usepackage{xspace}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{\arg\!\max}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\average}{Average}

\def\holfour{\textsf{HOL4}\xspace}
\def\isabelle{\textsf{Isabelle}\xspace}
\def\hollight{\textsf{HOL Light}\xspace}
\def\mizar{\textsf{Mizar}\xspace}
\def\coq{\textsf{Coq}\xspace}
\def\ocaml{\textsf{OCaml}\xspace}
\def\vampire{\textsf{Vampire}\xspace}
\def\eprover{\textsf{E-prover}\xspace}
\def\zthree{\textsf{z3}\xspace}

\def\sml{\textsf{SML}\xspace}
\def\polyml{\textsf{Poly/ML}\xspace}
\def\holyhammer{\textsf{HOL(y)Hammer}\xspace}
\def\sledgehammer{\textsf{Sledgehammer}\xspace}
\def\mizAR{\textsf{MizAR}\xspace}

\def\metis{\textsf{Metis}\xspace}
\def\leancop{\textsf{leanCoP}\xspace}
\def\tactictoe{\textsf{TacticToe}\xspace}
\newcommand{\bq}[1]{\textbackslash"{#1}\textbackslash"}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
%\theoremstyle{remark}
%\newtheorem{example}{Example}
%\newtheorem{remark}{Remark}
%\newtheorem{definition}{Definition}
\clubpenalty=10000
\widowpenalty=10000

\usepackage{fancyvrb}
\usepackage{tikz} % graphics
% graphs
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{trees,positioning,fit}
\tikzstyle{block} = [rectangle, draw,% fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line}=[draw]
\tikzstyle{cloud} = [draw, ellipse,%fill=red!20,
    node distance=3cm,
    minimum height=2em]


% graphs
\usetikzlibrary{arrows,shapes,calc}
\usetikzlibrary{trees,positioning,fit}
\tikzstyle{arrow}=[draw,-to,thick]
\tikzstyle{bluearrow}=[draw,-to,thick,blue]
\tikzstyle{tcircle} = [circle, draw, fill=white]
\tikzstyle{tsquare} = [rectangle, draw, fill=white]
\tikzstyle{bcircle} = [circle, draw, blue, fill=blue]
\tikzstyle{gcircle} = [circle, draw, mygreen, fill=mygreen]

%\title{TacticToe: A theorem prover trained from formal proofs}
%\title{TacticToe: Smart guidance in tactical proof search}
%\title{TacticToe: Proof  by Tactic Selection}
\title{TacticToe: Learning to Prove with Tactics}
%\title{TacticToe: Automatic tactic-level proof generation}

\author{\mbox{Thibault Gauthier} \and \mbox{Cezary Kaliszyk} \and \mbox{Josef
Urban} \and \mbox{Ramana Kumar} \and \mbox{Michael Norrish}}

\authorrunning{Gauthier et al.}
%\titlerunning{TacticToe: Learning the Game of Tactics}

\institute{Thibault Gauthier and Cezary Kaliszyk \at
Department of Computer Science, University of Innsbruck,
%Technikerstr. 21a/2,
Innsbruck, Austria\\ \url{{thibault.gauthier,cezary.kaliszyk}@uibk.ac.at}
\and
Josef Urban \at Czech Technical University, Prague\\\url{josef.urban@gmail.com}
\and Ramana Kumar and Michael Norrish \at Data61}


%\institute{
%  University of Innsbruck\\
%  \email{\{thibault.gauthier,cezary.kaliszyk\}@uibk.ac.at}
%\and
%  Czech Technical University, Prague.\\
%  \email{josef.urban@gmail.com}\\
%}


\usepackage[final]{listings}
%\usepackage[scaled=.95]{couriers}
\lstdefinelanguage{SML}%
{columns=fullflexible,
  keywords={THEN,THENL,val,open,store_thm,fun,fn,let,in,end},%
  frame=none,
  sensitive=true,
  keywordstyle=\fontfamily{lmss}\footnotesize\selectfont,%
  basicstyle=\fontfamily{pcr}\selectfont,%
  stringstyle=\tt,
  morestring=[b]",
  literate={\ }{$\mkern6mu$}1%
   {=}{{\tt\raisebox{-.15mm}{=}}}1%
   {[}{{\tt\raisebox{-.15mm}{[}}}1%
   {]}{{\tt\raisebox{-.15mm}{]}}}1%
   {->}{{$\rightarrow$}}1%
   {==>}{{$\Rightarrow$}}1%
   {wedge}{{$\wedge$}}1%
   {>=}{{$\geq$}}1%
   {'a}{{$\alpha$}}1%
   {'b}{{$\beta$}}1%
   {ldots}{$\ldots$}1%
   {!}{$\forall$}1%
   {``}{\hspace{-.5mm}`\hspace{-1mm}`}1%
  }

\lstdefinelanguage{SMLSmall}%
{columns=fullflexible,
  keywords={THEN,THENL,val,open,store_thm,fun,fn,let,in,end,true,
  while,do,if,then,else,break,return,;},%
  frame=none,
  sensitive=true,
  keywordstyle=\fontfamily{lmss}\scriptsize\selectfont,%
  basicstyle=\fontfamily{pcr}\small\selectfont,%
  stringstyle=\tt,
  morestring=[b]",
  literate={\ }{$\mkern6mu$}1%
   {=}{{\tt\raisebox{-.15mm}{=}}}1%
   {[}{{\tt\raisebox{-.15mm}{[}}}1%
   {]}{{\tt\raisebox{-.15mm}{]}}}1%
   {->}{{$\rightarrow$}}1%
   {==>}{{$\Rightarrow$}}1%
   {wedge}{{$\wedge$}}1%
   {<=}{{$\leq$}}1%
   {>=}{{$\geq$}}1%
   {'a}{{$\alpha$}}1%
   {'b}{{$\beta$}}1%
   {ldots}{$\ldots$}1%
   {emptyset}{$\emptyset$}1%
   {!}{$\forall$}1%
   {fff}{{DB.fetch}}1%
   {``}{\hspace{-.5mm}`\hspace{-1mm}`}1%
  }
\lstset{language=SML}
\begin{document}
\maketitle

\todoi{JU review: Still feels more like an intro than an abstract. Could be
shorter and punchier.}

\todoi{Move part of the abstract to the introduction.}
\begin{abstract}
Formalization of mathematics in interactive theorem provers (ITPs) relies today on many proof automation methods.
Some of these methods are general-purpose and with enough time they can prove any provable conjecture.
Many formalized proof scripts however also take advantage of specialized rules and theory-based strategies.
In the HOL4 ITP system, practically all of them are implemented as \emph{tactics}.

Automated theorem provers (ATPs) rely on parameterizable general
strategies. Sometimes, a strategy can be specialized for only one theory as in
SMT solving. But it is difficult for an ATP to rely on rules tailored for a particular case.
Indeed, this would require implementing many specific rules and more
importantly figuring out in which cases they can be applied.

We bridge this gap by implementing our tactical prover TacticToe on top
of the HOL4 ITP.
TacticToe first learns from many proof scripts
the relevance of tactics in particular proof situations.
This learned knowledge is then used in a Monte Carlo tree search algorithm that explores the most promising tactic-level proof paths.
On a single CPU, with a time limit of 60 seconds, TacticToe proves 66.4\%
of the 7164 theorems in HOL4's standard library whereas
Eprover solves 34.5\%. The success rate rises to 69.0\% by combining the
results of both provers.
\end{abstract}

\tableofcontents
\section{Introduction}
Many of the state-of-the-art interactive theorem provers (ITPs) such as
  \holfour~\cite{hol4}, \hollight~\cite{Harrison09hollight},
  \isabelle~\cite{isabelle}
  and \coq~\cite{coq-book} provide high-level parameterizable tactics for constructing proofs.
  Such tactics typically analyze the current proof state (goal and
  assumptions), then apply non-trivial proof transformations that get
  expanded into many basic kernel-level inferences or large parts of a proof term.
%\marginpar{not Coq}
  In this work we develop a tactic-level automation procedure for the \holfour ITP
  which guides the selection of tactics by learning from previous
  proofs.  Instead of relying on translation to first-order automated
  theorem provers (ATPs) as done by the hammer
  systems~\cite{hammers4qed,tgck-cpp15}, our technique
%\marginpar{too many cites?}
%BlanchetteGKKU16,holyhammer,mizAR40
  searches directly for sequences of tactic applications that lead to
  an ITP proof, thus avoiding the translation and proof-reconstruction phases needed by the hammers.

  To do this, we \emph{extract and record} tactic invocations from the ITP
  proofs (Section~\ref{sec:recording}) and
  \emph{build efficient machine learning classifiers} based on such training
  examples (Section~\ref{s:prediction}).  The learned data serves as
  guidance for a \emph{Monte Carlo tree search} that explores different
  proof paths (Section~\ref{sec:proofsearch}). The result, if
  successful, is a verified human-level proof composed of \holfour
  tactics.  The system is evaluated on a large set of theorems originating
  from \holfour (Section~\ref{s:experiments}).

\subsection{The problem}
% How a user works with an ITP and how this gives rise to a machine learning problem.
\todo{RK: this is just a first draft -- feel free to edit lots!}
An ITP is a development environment for the construction of formal proofs in which it is possible, but not common, to write a proof as a sequence of applications of primitive inference rules.
The common approach, however, is to use and devise high-level tools and automation that abstract over useful ways of producing proof pieces.
A specific, prevalent instance of this approach is the use of \emph{tactics} for \emph{backward} or goal-directed proof.
Here, the ITP user operates on a proof state, initially the desired conjecture, and applies tactics that transform the proof state until it is solved (or until giving up).
Each tactic performs a sound transformation of the proof state: essentially, it promises that if the resulting proof state can be solved, then so can the starting one.
This gives rise to a machine learning problem: can we learn a mapping from a given proof state to the next tactic (or sequence of tactics) that will productively advance the proof towards a solution?

\paragraph{Goals and theorems.}
A goal is represented as a \emph{sequent}.
A sequent is composed of a set of assumptions and a conclusion, all of which are higher-order logic formulas.
When a goal is proven, it becomes a theorem.
If the developer gives the theorem a name then we refer to it as a top-level theorem.

\paragraph{Theories and script files.}
Formal developments in \holfour are organized into named theories, which are
concretely implemented by \emph{script} files defining modules in the
functional programming language \sml.
Each script file corresponds to a single theory, and contains definitions of
types and constants as well as statements of theorems together with their
proofs.
In practice, tactic-based proofs are written in an informally specified
\emph{embedded domain-specific language}: the language consisting of
pre-defined tactics and \emph{tacticals} (functions that operate on tactics).
However, the full power of \sml is always available for users to define their
own tactics or combinations thereof.
%To automate proof search strategies a user can implement programming language functions that correspond to more involved proofs.
%Functions that correspond to backward proofs, called tactics, are combined with each other with the help of tacticals (functions that takes
%tactics as arguments).
%Such a combination of tactics will be called a \emph{proof script} when it proves a theorem stated in a theory.

\paragraph{Tactics in \holfour.}
Considering concrete implementation details now, a \emph{tactic} is a function
that takes a goal (or proof obligation) and returns a list of goals (subgoals
of the original goal) together
with a validation function.
Calling the validation function on a list of \emph{theorems}, corresponding to
the list of subgoals, results in a theorem corresponding to the original goal.
For example, if the list of subgoals is empty, then calling the validation
function on the empty list yields the originally goal sequent as a theorem.
In this way, tactics implement the plumbing of backwards, goal-directed, proof
construction.
Because validation functions are only important to check the final proof, we
omit them during the description of proof search algorithm. We denote by $t(g)$
the list of goals produced by the application of a tactic $t$ to a goal $g$.

\subsection{Contributions}
This paper extends the work described in \cite{tgckju-lpar17}.
There we proposed the idea of a tactic-based prover based on supervised
learning guidance. And we achieved a 39\% success rate on theorems of the
\holfour standard library by running \tactictoe with a 5 seconds timeout.
The additional contributions and their effect on our system are:

\begin{itemize}
\item Proof recording at the tactic level is made more precise. Pattern
matching constructions, opening of submodules are supported. The different
recording steps (parsing, unfolding and replaying) are regrouped into a single
\sml function.
\item Monte Carlo tree search~\cite{montecarlo} (MCTS) replaces A* as our
search algorithm. The MCTS algorithm gives a more balanced feedback on
the success of
each proof step by
comparing subtrees of the search tree instead of leafs. The policy and
evaluation are learned
through supervised learning.
\item Proof guidance required by MCTS is given by predictors for
three kinds of objects: tactics, theorems and lists of goals.
\item The orthogonalization process that eliminates redundant tactics is
described in a precise manner.
\item A tactic abstraction mechanism is introduced. It enables us to create
more general and flexible tactics where tactic arguments are dynamically
predicted.
\item The internal ATP \metis is complemented
by asynchronous calls to \eprover to help \tactictoe during proof search.
\item Evaluation of \tactictoe with a 60 seconds timeout achieving a 66\%
success rate on the standard library.
\item First evaluation of \tactictoe success rate relative to
the length of the original human proofs.
\item Minimization and prettification of the generated proof improves user
interaction.
\end{itemize}


\section{Search tree}\label{sec:prelim}

The application of a tactic to an initial conjecture produces a list of goals.
Each of the created goals can in turn be the input to other tactics.
Moreover, it is possible to try multiple tactics on the same goal.
In order to keep track of the progress made so far from the initial conjecture we want to solve, we organize goals and tactics into a graph with list of goals as nodes and tactics as edges (See Definition~\ref{def:stree}).

In this section, we only give a description of the graph at a given
moment of the search after a certain amount of tactic applications.
Exploration of the tree is directed by the MCTS algorithm in
Section~\ref{sec:proofsearch}, which is guided by prediction algorithms
presented in Section~\ref{sec:predictions}.


\begin{definition}\label{def:stree}(search tree)\\
A search tree $\mathfrak{T}$ is a sixtuple
$(\mathbb{T},\mathbb{G},\mathbb{A},T,G,A)$
that respects the following conditions:
\begin{itemize}
\item $\mathbb{T}$ is a set of tactics, $\mathbb{G}$ is a set of goals
 and $\mathbb{A}$ is a set of nodes representing lists of goals.
\item $T$ is a function from $\mathbb{G}$ to $\mathcal{P}(\mathbb{T})$. It
takes a goal and return the set of tactics already applied to this goal.
\item $G$ is a total function from $\mathbb{A}$ to $\mathcal{P}(\mathbb{G})$.
It takes a node and returns the list of goals of this node.
\item $A$ is a partial function from $\mathbb{G} \times \mathbb{T}$ to
$\mathbb{A}$. It takes a goal and a tactic and returns the node produced by the
application of the tactic to the goal. Its support $\mathit{Support}(A)$ is
defined as $\lbrace (g,t) \in \mathbb{G} \times \mathbb{T}\ |\ t \in T(g)
\rbrace$. And if $(g,t) \in \mathit{Support}(A)$, then $t(g) = G(A(g,t))$. This
means that the output $t(g)$ is exactly the list
of goals contained in the node produced by
$t$ from $g$.
\end{itemize}

Acyclicity and the fact that there is exactly
one root node containing only the initial goal are additional constraints to
the search tree that are preserved during the
application of the MCTS algorithm. That is why we call it a tree, although this
is not specified in this definition.
\end{definition}

If no explicit order is given, we assume that the sets
$\mathbb{T},\mathbb{G},\mathbb{A}$ are equipped with an
arbitrary order. In Figure~\ref{fig:choice}, a part of the a search tree is
depicted, with goals represented by circles, nodes by rectangles and tactics by
arrows.


\begin{figure}{}
\begin{center}
\begin{tikzpicture}[node distance=0.7cm]
\node [tcircle] (0) {$g_i$};
\node [right of=0] (0r) {.\ .\ .};
\node [tcircle,right of=0r] (0rr) {$g_n$};
\node [left of=0] (0l) {.\ .\ .};
\node [tcircle,left of=0l] (0ll) {$g_0$};
\node [draw,fit=(0rr) (0ll)] (0b) {};
\node [left of=0ll] {$a_0$};
\node [above of=0,node distance=2cm] (2) {.\ .\ .};
\node [left of=2, tcircle] (2l) {$\phantom{g_3}$};
\node [tcircle,right of=2] (2r) {$\phantom{g_3}$};
\node [draw, fit=(2l) (2r)] (2b) {};
\node [left of=2l] {$a_j$};

\node [tcircle, left of=2l, node distance=3cm] (1r) {$\phantom{g_3}$};
\node [left of=1r] (1) {.\ .\ .};
\node [tcircle, left of=1] (1l) {$\phantom{g_3}$};
\node [draw, fit=(1l) (1r)] (1b) {};
\node [left of=1l] {$a_1$};

\node [right of=2r,node distance=3cm] (3l) {$\phantom{g_3}$};
\node [right of=3l] (3) {\phantom{.\ .\ .}};
\node [right of=3] (3r) {$\phantom{g_3}$};
\node [fit=(3l) (3r)] (3b) {};

\node [fit=(1r) (2l)] (1m) {};
\node [fit=(2l) (3r)] (2m) {};

\draw[-to,thick] (0) to node[xshift=-10](t1){$t_1$} (1b);
\draw[-to,thick] (0) to node[xshift=-4](t2){$t_j$} (2b);
\draw[-to,dotted,thick] (0) to (1m);
\draw[-to,dotted,thick] (0) to (2m);
\draw[-to,dotted,thick] (0) to node[xshift=15](t2){$t_m$} (3b);
\end{tikzpicture}
\end{center}
\caption{A node of a search tree $a_0$, the list of goals $g_0 \ldots g_n$ it
contains and the nodes $a_1 \ldots a_m$ derived from the application of the
tactics $t_1 \ldots t_m$ to $g_i$.}
\label{fig:choice}
\end{figure}


In the following, properties are defined in the context of a
search tree $\mathfrak{T}$. In Definition~\ref{def:solved}, we specify
different status for a goal: open, pending or solved. There, we define
what a solved goal and solved node are by mutual recursion on the number of
steps it takes to solve a goal and a node.
\begin{definition}\label{def:solved}(Solved goal, solved nodes, open goal,
pending goal)\\
The set of solved nodes $\mathbb{A}^\infty$ and
the set of solved goals $\mathbb{G}^\infty$ are defined inductively by:

\begin{align*}
\mathbb{A}^{0} &=_{def}
\lbrace a \in \mathbb{A}\ |\ G(a) = \emptyset \rbrace \\
\mathbb{G}^{0} &=_{def} \lbrace g \in \mathbb{G}\ |\
\exists t \in T(g).\ A(g,t) \in \mathbb{A}^{0} \rbrace\\
\mathbb{A}^{n+1} &=_{def} \lbrace a \in \mathbb{A}\ |\
\forall g \in G(a).\ g \in \mathbb{G}^{n} \rbrace\\
\mathbb{G}^{n+1} &=_{def} \lbrace g \in \mathbb{G}\ |\
\exists t \in T(g).\ A(g,t) \in \mathbb{A}^{n+1} \rbrace \\
\mathbb{A}^\infty &=_{def} \bigcup_{n \in \mathbb{N}} \mathbb{A}^n \ \ \ \ \
\mathbb{G}^\infty =_{def} \bigcup_{n \in \mathbb{N}} \mathbb{G}^n\\
\end{align*}

We call a goal (respectively node) unsolved if it is not in the set of solved
goal (respectively node).
The open goal of a node is the first unsolved goal of this node according to
some preset order. The other unsolved nodes are called pending nodes.
If a node is unsolved, then it contains a unique open goal.
During MCTS exploration, we do not explore pending goals of an unsolved node
before its open goal is solved which justifies our terminology. In other words,
our search tree has the following property: if $g$ is a pending node then
$T(g)$ is empty.

\end{definition}


In Definition~\ref{def:desc}, we define relations between goals
and nodes in terms of parenthood.

\begin{definition}\label{def:desc}(children, descendant, ancestors)\\
The children of a goal $g$ is a set of nodes defined by:
\[\mathit{Children}(g) = \lbrace A(g,t)\in \mathbb{A}\ |\ t \in T(g) \rbrace\]

By extension, we define the children of a node $a$ by:
\[\mathit{Children}(a) = \bigcup_{g \in G(a)} \mathit{Children}(g) \]

The descendants of $a$ by:
\begin{align*}
\mathit{Descendant}^{0}(a) &=_{def} \lbrace a \rbrace \\
\mathit{Descendant}^{n+1}(a) &=_{def} \bigcup_{a' \in \mathit{Descendant}^{n}(a)}
\mathit{Children}(a') \\
\mathit{Descendant}(a) &=_{def} \bigcup_{n \in \mathbb{N}} \mathit{Descendant}^n(a)\\
\end{align*}

And the ancestors of $a$ by:
\[\mathit{Ancestors}(a) =_{def} \lbrace b \in \mathbb{A} \ | \ a \in
Descendant(b) \rbrace\]

\end{definition}


In Definition~\ref{def:prod}, we judge if a tactic is productive from its
contribution to the search tree.

\begin{definition}\label{def:prod} (Productive tactic)
The application of a tactic on an open goal is called productive if and only if
all the following conditions are satisfied:
\begin{itemize}
\item It does not fail or timeout. To prevent tactics from looping, we
interrupt tactics after a short amount of time (0.05 seconds).
\item It does not loop, i.e. its output does not contain any goals that appears
in the ancestor of the node of the current goal.
\item It is not a parallel step, i.e. its output is not a superset
of a list of goals produced from the same goal.
\end{itemize}

The third point of the definition is a partial attempt at preventing confluent
branches. The general case where two branches join after $n$ steps is handled
by a tactic cache which memorizes tactic effects on goals. This
cache allows faster re-exploration of confluent branches which remain
separated in the search tree.
\end{definition}


\section{Prediction}\label{s:prediction}
The learning-based selection of relevant lemmas significantly improves the
automation for hammers~\cite{BlanchetteGKKU16}. In \tactictoe we use the
distance-weighted \emph{$k$ nearest-neighbour} classifier~\cite{DudaniS76}
adapted for lemma selection~\cite{ckju-pxtp13}. It allows for hundreds of
predictions per second, while maintaining very high prediction quality~\cite{femalecop}.
We rely on this supervised machine learning algorithm to predict three
different kinds of objects:
 tactics, theorems and lists of goals. The number of predicted objects $k$ is
 called the \emph{prediction radius}. For a goal $g$, we note
$\mathit{Predictions}^{\mathit{tactic}}_k (g)$ (respectively
$\mathit{Predictions}^{\mathit{theorem}}_k (g)$
and $\mathit{Predictions}^{\mathit{goal\_list}}_k (g)$) the $k$ tactics
(respectively theorems and lists of goals) selected by our prediction algorithm.

We introduce specificities associated with the prediction of each object.
In particular, we present the dataset from which objects are selected and the
purpose of the selection process. Details on the similarity measure backing the
predictions is described in Section~\ref{sec:features} and two methods for
improving the quality of the predictions are presented in
Section~\ref{sec:ortho} and Section~\ref{sec:synthesis}.

\paragraph{Tactics}
To start with, we build a database of tactics consisting
of tactic-goal pairs recorded from successful tactic applications in human
proofs (see
Section~\ref{sec:recording}).
Then, during proof search, we order recorded goals according to their
similarity with a targeted goal $g$ (usually the first pending goal of a node).
The recorded pairs induce an
order on tactics. The intuition is that tactics which have been successful on
goals similar to $g$ should be tried first, since they are more likely to lead
to a proof.
This predicted tactic order is translated into a prior policy for our MCTS
algorithm (see Section~\ref{sec:policy}).
Tactic selection is also used to improve the quality of the database
of tactics during orthogonalization (see Section~\ref{sec:ortho}).

\paragraph{Theorems as Arguments of Tactics}
We first collect a set of theorems for our predictor to select from.
It includes the \holfour theorem database and theorems from the local namespace.
Predicted tactics during proof search and orthogonalization may include
tactics where arguments (lists of theorems) have been erased (see Section
\ref{sec:synthesis}).
We instantiate those tactics by theorems that are syntactically the closest to
the goal as they are more likely to help.
The same algorithm selects suitable premises for ATPs integrated with
\tactictoe (see Section~\ref{sec:atp}).

\paragraph{Lists of Goals as Output of Tactics}
A dataset of tactic outputs is compiled during orthogonalization.
This set is separated into positive and negative examples (see
Section~\ref{sec:evaluation}).
List of goals that are a subset of the result of a recorded tactic on a
targeted goal are considered positive. During proof search, given a list of
goals $l$ created by a tactic, we select a set of lists of goals that are most
similar to $l$. The ratio of positive examples in the selection gives us a
prior evaluation for MCTS.

\section{Proof Search}\label{sec:proofsearch}

Despite the best efforts of the prediction algorithms, a selected tactic may
not solve the current goal, proceed in the wrong direction
or even loop. For that reason, predictions need to be accompanied by a
proof search mechanism that allows for backtracking and
can choose which proof tree to extend next and in which direction.
Our search algorithm is a Monte Carlo tree search~\cite{montecarlo} algorithm
that relies
on a prior evaluation function and a prior policy function. Those priors are
learned through direct supervised learning for the policy and via the data
accumulated during orthogonalization for the evaluation. Therefore, this MCTS
algorithm does not need to rely on roll outs for evaluation. The
policy and evaluation are estimated by the simple supervised k-NN algorithm
here instead of neural networks trained through reinforcement learning in
AlphaGo Zero~\cite{silver2017mastering}.
A characteristic of the MCTS algorithm is that it offers a good trade-off
between exploitation and exploration. This means that the algorithm searches
deeper more promises branches and leaves enough time for the exploration
of less likely alternatives.


We first describe the proof search algorithm and explain later how to compute
the prior policy \texttt{PriorPolicy} and prior evaluation
\texttt{PriorEvaluation} functions.

\subsection{Proof exploration}

The proof search starts by creating an initial search tree. The search tree
then evolves by the repetitive applications of MCTS steps. A step in the main
loop of MCTS consists of three parts: node selection, node extension, and
backpropagation. And the decision to terminate the algorithm is taken during
resolution.

\paragraph{Initialization}
The input of the algorithm is a goal $g$ (also called conjecture) that we
want to prove.
Therefore the search tree starts with only one node the list of goals
$[g]$ and an ordered list of predicted tactics for the goal $g$.
%Then, the MCTS algorithm alternates node selection steps and node extension
%steps which grow the search tree.


%A similar trade-off between exploration and exploitation exists in algorithm
%like Monte Carlo. However, Monte Carlo tree search metrics are based on many
%simulations and do not reflect our exploration strategy that only
\paragraph{Node selection}
Through node extension steps the search tree grows and the number of paths to
explore increases. To decide which node to extend next, a evolving value for
each node is given in Definition~\ref{def:value}. The algorithm that performs
node selection starts from the root of the search tree. From the current node,
the function \texttt{CompareChildren} choses among the children of its open
goal the one with the highest value.

If the highest children value is higher than the widening policy, then the
selected child
becomes the current node, otherwise the final result of node selection is the
current node. The node selection algorithm can be written as the following
pseudo-code:

\begin{lstlisting}[language=SMLSmall]
CurrentNode = Root(Tree);
while true do
  if Children(CurrentNode) = emptyset then break;
  (BestChild, BestValue) = CompareChildren (CurrentNode);
  if WideningPolicy (ChosenNode) >= BestValue then break;
  CurrentNode = BestChild
end;
return CurrentNode;
\end{lstlisting}

\begin{definition}\label{def:value}(Value)
The value of the $i^th$ child $a_i$ of the open goal of a parent node $p$
is determined by:
\[\mathit{Value}(a_i) = \mathit{CurEvaluation}(a_i) + c_{exploration} *
\mathit{Exploration}(a_i)\]

The number $\mathit{Failure}(a_i)$ is the amount of failures that occurred
during node extension from descendants of $a_i$ .
The current evaluation $CurEvaluation(a_i)$ is the average evaluation of
all descendants of $a_i$ including node extension failures.

\[\mathit{CurEvaluation}(a_i) =
  \sum_{a' \in Descendant(a_i)} \frac{\mathit{PriorEvaluation}(a')} {card\
  \mathit{Descendant}(a_i) + \mathit{Failure}(a_i)}\]


The exploration term is determined by the prior policy and the current policy.
The current policy is calculate from the number of times a node $x$ has been
traversed during node selection, noted $\mathit{Visit}(x)$.

\[\mathit{Exploration}(a_i) =
\frac{\mathit{PriorPolicy}(a_i)}{\mathit{CurPolicy}(a_i)}\]

\[\mathit{CurPolicy}(a_i) = \frac{1 +
\mathit{Visit}(a_i))}{\sqrt{\mathit{Visit}(p))}}\]
The policy is approximatively the  percentage of time a node was visited.The
square root skews this probability to favor even more exploration of nodes with
few visits.

The coefficient $c_{exploration}$ is experimentally determined and adjusts the
trade-off between exploitation and exploration.
\end{definition}

\paragraph{Node extension}
Let $a$ be the result of node selection.
If $a$ is a solved node or the descendant of a solved node, then extending $a$
is useless and the algorithm reports a failure for this MCTS step.
%In our
%experiments, filtering such nodes before node selection is detrimental.
If $a$ is not $solved$, it applies the best untested tactic $t$ on the open
goal $g$ of this
node according to the prediction order for $g$. If no such tactic exists or if
$t$ is not productive, a failure is returned.
If node extension succeeds, a new node containing $t(g)$ is added to the
children of $a$.


\paragraph{Backpropagation}
During backpropagation we update the statistics of all the nodes traversed or
created during this MCTS step:
\begin{itemize}
\item Their number of visit is incremented.
\item If node extension failed, their failure count is incremented.
\item If node extension succeeded, they inherit the evaluation of the created
child.
\end{itemize}

These changes updates the current evaluation and the current policy of the
traversed nodes.
After completing backpropagation, the process loops back to node
selection.

\paragraph{Resolution}
The search ends when the algorithm reaches one of these 3 conditions:
\begin{itemize}
\item It finds a proof, i.e. the root node is solved. In this case,
the search returns a minimized and
prettified proof script (see Section~\ref{sec:proofdisplay}).
\item It saturates, i.e. there is no tactics to be applied to any open goal.
This happens less than 10 times in the full-scale experiment.
\item It exceeds a given time limit set by default to 60 seconds.
\end{itemize}

\subsection{Supervised learning guidance}
Our MCTS algorithm relies on the guidance of two supervised learning methods,
the prior policy decides the order in which tactic should be tried and the
prior evaluation estimates if a branch is fruitful by analysing tactic
outputs. Both priors influence the rates at which branches of the search tree
are explored.

\paragraph{Prior policy}\label{sec:policy}
Co-distances between goals produced by the tactic predictor are hard
to translate into a prior policy, so we rely solely on the induced order of
tactics to estimate the probability with which each tactic should be tried.
Let $\mathfrak{T}$ be a search tree after a number of MCTS steps.
Let $p$ be a selected node in $\mathfrak{T}$ and $g$ its open goal. We order
the list of $n$ productive tactics already
applied to $g$ by their similarity score so that $T(g)= \lbrace
t_0,\ldots,t_{n-1} \rbrace$.
Let $c_{policy}$ be a constant heuristic that estimates how likely a tactic
$t_i$ is to prove the goal.
We can now calculate of a prior policy of a child $a_i$ produced by the tactic
$t_i$ by:
\[PriorPolicy(a_i) = (1 - c_{policy})^{i} * c_{policy}\]

In order to include the possibly of trying more tactics on $g$ we define the
widening policy on the parent $p$ for its open goal $g$ to be:

\[WideningPolicy(p) = (1 - c_{policy})^{n} * c_{policy}\]


\paragraph{Prior evaluation}\label{sec:evaluation}

We now concentrate on the definition a reasonable evaluation function for the
output of tactics. Theoretically, an interesting list of goals is one
that has short proofs for each of its goals, and we could estimate their
lengths from previous proof searches. However, this
approach generates too much data and slows down the proof search. So, we only
collect output of tactics tested during orthogonalization. We
declare a list of goals $l$ to be positive if it is has been produced by the
winner of an orthogonalization competition.
The set of positive examples in
$\mathit{Predictions}^{\mathit{goal\_list}}_k(l)$ is noted
$\mathit{Positive}^{\mathit{goal\_list}}_k(l)$.
We chose $k=10$ as a default evaluation radius in our experiments.
And we evaluate a node $a$ through the list of goals $G(a)$ it
contains using the prior evaluation function:
\begin{align*}
\mathit{PriorEvaluation}_k (a) &=
  \frac{card\ \mathit{Positive}^{\mathit{goal\_list}}_k(G(a)}{k}\\
\end{align*}


\subsection{ATP Integration}~\label{sec:atp}
General-purpose proof automation mechanisms which combine proof translation to
ATPs with machine learning (``hammers'') have become quite successful in
enhancing the automation level in proof assistants~\cite{hammers4qed}.
As external automated reasoning techniques sometimes outperform the combined
power of tactics, we would like to combine the \tactictoe search with
general purpose provers such as the ones found in \holyhammer for
\holfour~\cite{tgck-cpp15}.

Such a prover already exists in \holfour and is called \metis. It is already
recognized by the tactic selection mechanism in \tactictoe and can have its
premises predicted dynamically when in an abstracted form. Nevertheless,
we think that the performance of \tactictoe can be boosted by giving the ATP
\metis a special status. This arrangement consists of always predicting
premises for \metis, giving it a slightly higher timeout and
trying it first on each open goal. These modifications only induce an
linear overhead on the number nodes as a general purpose prover does not create
goal obligations.

The integration of an external prover is also possible. And
\eprover~\cite{eprover} is the one we experimented with.
To this end, we created a tactic which expects a number of premises as an
argument
and calls \eprover on the translated first order problem. We then extract the
premises that were required in the \eprover
proof and reconstruct it inside \holfour with \metis. Giving a special status
to external
provers is essential as external ATP calls do not appear in human
proof scripts. For external ATPs, we use an even higher timeout (5 seconds) and
a larger number of predicted premises (128 for \eprover) and also try them
first on open
goals. Since calls to ATPs are
computationally expensive, they are run in parallel and asynchronously. This
avoids slowing down \tactictoe's search loop. The number of asynchronous calls
to external provers that can be executed at the same time is limited by the
number of cores available to the user.





\section{Feature Extraction}\label{sec:features}
We explain the construction of the similarity measure by which relevant
objects are predicted

We predicts tactics through their associated goals. So, the features
for each kind of object can be extracted from (different representations of)
mathematical formulas. We start by describing features for \holfour terms. From
there, we extend the extraction mechanism
to goals, theorems and lists of goals. Duplicated features are always removed
so that each object has an associated set of features.

Here are the different kind of feature we extract from terms:
\begin{itemize}
\item names of constants, including the logical operators,
\item type constructors present in the types of constants and variables,
\item subterms with all variables replaced by a single placeholder $V$,
\item names of the variables.
\end{itemize}
Goals (respectively theorems) are represented by a couple of a list of
terms (assumptions) and a term (conclusion). So, we distinguish between
features of the assumptions and features of the conclusion by adding a
different tag to elements of each set. The union of all those
features are considered the feature of a goal (respectively theorem). From
that, we can construct features for a list of goals by
from the union of the features of each goal in the list.

\subsection{Similarity}\label{sec:predictions}
We estimate the similarity (or co-distance) between two objects $o_1$ and $o_2$
through their respective feature sets $f_1$ and $f_2$.
The estimation is based on the frequency of the features in the intersection of
$f_1$ and $f_2$. A good rule of thumb is that the rarer the shared features
are, the more similar two goals should be. The relative rarity of each feature
can be estimated by calculating its TF-IDF weight~\cite{Jones72astatistical}.
We additionally modify the weight by raising them to the sixth power giving
even more credence
to rare features. This modification was found experimentally optimal in
\cite{ckju-pxtp13}.

The first co-distance $sim_1$ sums the weight of each shared
features to compute the total score.
In a second co-distance $sim_2$, we additionally take
the total number of features into account, to reduce the seemingly unfair
advantage of big
feature sets in the first scoring function.
\[sim_1 (o_1, o_2) = {\sum\nolimits_{\,f \in f_0 \cap
<f_1}{\text{tfidf}(f)^{6}}}\]
\[sim_2 (o_1, o_2) = \frac{{\sum\nolimits_{\,f \in f_0 \cap
f_1}{\text{tfidf}(f)^{6}}}}
{ln (e + \mathit{card} f_0 + \mathit{card} f_1)}\]

In our setting, making predictions for an object $o$ consists of sorting a set
of objects by their similarity to $o$. We use $sim_1$ for tactics and theorems
and $sim_2$ for lists of goals. The reason why $sim_1$ is used for
predicting tactics is that tactics effective on a large goal $g$ are often also
suitable for a goal made of sub-formulas of $g$. The same monotonicity
heuristic can justify the use of $sim_1$ for theorems. Indeed, in practice a
large theorem is often a conjunction of theorems from the same domain.
And if a conjunction contains a formula related to a problem, then the other
conjuncts from the same domain may also help to solve it.

\subsection{Preselection}\label{sec:dependencies}

In order to speed up the predictions during the search for a proof of a
conjecture $c$, we preselect 500 tactic-goal pairs and 500 theorems and a
larger number of lists of goals induced by the selection of tactic-goal pairs.
Preselected objects are the only objects available to \tactictoe's search
algorithm for proving $c$.

%During search, preselected tactics are reordered for
%each open goal creating the prior policy of our MCTS algorithm (see
%Section~\ref{sec:policy}).

The first idea is to select tactic-goal pairs and theorems by their similarity
with $c$ (i.e $\mathit{Predictions}^{\mathit{tactic}}_{500} (c)$ and
$\mathit{Predictions}^{\mathit{theorem}}_{500} (c)$).
However, this selection may not be adapted to later stages of
the proof where goals may have drifted significantly from $c$. In order to
anticipate the production of diverse goals, our prediction during preselection
takes dependencies between objects of the same dataset into account.
This dependency relation is assymetric. Through the relation, each object has
multiple children and at most one parent.
Once a dependency relation is established we can calculate a dependency score
for each object, which is the maximum of its similarity score
and the similarity score of its parent. In the end, the 500 entries with
highest dependency score are preselected in each dataset.

We now give a mathematical definition of the dependency relation for each kind
of object.

\begin{definition}(Dependencies of a tactic-goal pair)\\
Let $\mathfrak{F}$ be the set of recorded tactic-goal pairs.
The dependencies $D_\infty$ of a tactic-goal pair $(t_0,g_0)$ is
inductively defined by:

\begin{align*}
D_0 &=_{def} \lbrace (t_0,g_0) \rbrace \\
D_{n+1} &=_{def} D_n \cup \lbrace (t,g)\in \mathfrak{F}\  |\ \exists
(t',g') \in D_n.\ g \in t'(g') \rbrace  \\
D_\infty &=_{def} \bigcup_{i \in \mathbb{N}} D_i\\
\end{align*}
\end{definition}


\begin{definition}(Dependencies of a theorem)\\
Let $\mathfrak{T}$ be the set of recorded theorems.
The dependencies of a theorem $t$ are the set of top-level theorems in
$\mathfrak{T}$ appearing in the proof of $t$. These dependencies are
recorded by tracking the use of top-level theorems through the inference
kernel~\cite{tgck-cpp15}.
\end{definition}

\begin{remark}
The relation chosen for tactic-goal pairs is transitively closed, which is not
the case for theorem dependencies.
\end{remark}

To preselect a list of goal $l$ from our database of lists of goals, we
consider the tactic input $g$ that was at the origin of the production of $l$
during orthogonalization. The list $l$
is preselected if and only if $g$ appears in the 500 preselected tactic-goal
pairs. This has the
desirable effect that negative and positive examples derived from the same
input goal are chosen or discarded together.
\todo{this paragraph was confusing for Lasse}

\section{Orthogonalization}\label{sec:ortho}
Different tactics may transform a single goal in the same way. Exploring such
equivalent paths
is undesirable, as it leads to inefficiency in automated proof search.
To solve this problem, we modify how the database of tactics is build.
Each time a new tactic-goal pair $(t,g)$ is extracted from a tactic proof and
about to be recorded, we consider if there does not exists already a better
tactic for $g$ in our database. To this end, we organize a competition between
the
$k$ closest tactic-goal pairs $\mathit{Predictions}^{\mathit{tactic}}_k(g)$.
In our experiments,
the default orthogonalization radius is $k=20$.
The winner is the tactic that subsumes (see Definition~\ref{def:tacsub}) the
original tactic on $g$ and that
appears in the largest number of tactic-goal pairs in the database.
The winning tactic $w$ is then associated with $g$, producing the pair $(w,g)$
and is stored in the database instead of the original pair $(t,g)$.
As a result, already successful tactics with a large coverage are preferred,
and new tactics are considered only if they provide a different contribution.
We now give a formal definition of the concepts of subsumption and coverage
that are required for expressing the orthogonalization algorithm.

\begin{definition} (Coverage)\\
Let $\mathfrak{T}$ be the database of tactic-goal pairs. We can define the
coverage $\textit{\mbox{Coverage}}(t)$ of a tactic $t$ by the number of times
this tactic
appears in
$\mathfrak{T}$. Expressing this in a formula, we get:
  \[\textit{\mbox{Coverage}}(t) =_{def} card\ \lbrace g\ |\ (t,g) \in
  \mathfrak{T} \rbrace \]
Intuitively, this notion estimates how general a tactic is by counting the
number of different goals it is useful for.
\end{definition}


\begin{definition} (Goal subsumption)\\
A goal subsumption $\le$ is a partial relation on a set of goals.\\
Assuming we have a way to estimate the number steps required to solve a goal,
a possibly useful subsumption definition could be:
\[g_1 \le g_2  \Leftrightarrow_{def} g_1 \mbox{ has a shorter proof than }
g_2\]
By default, we choose a minimal subsumption defined by:
\[g_1 \le g_2  \Leftrightarrow_{def} g_1 \mbox{ is }\alpha\mbox{-equivalent
to } g_2\]
\end{definition}

We can naturally extend a goal subsumption to lists of goals.
\begin{definition} (Goal list subsumption)\\
A goal list subsumption is an partial relation on lists of goals.
Given two lists of goals $l_1$ and $l_2$, we can define it from goal
subsumption $\le$ by:
\[l_1 \le l_2  \Leftrightarrow_{def} \forall g_2 \in l_2.\ \exists g_1 \in l_1.\
g_1 \le g_2\]
\end{definition}

This allows us to define subsumption for tactics.
\begin{definition}\label{def:tacsub}(Tactic subsumption)\\
Given two non-failing tactics $t_1$ and $t_2$ on $g$, a tactic $t_1$ subsumes a
tactic $t_2$ on a goal $g$, noted $\le_g$, when:

 \[t_1 \le_g t_2 \Leftrightarrow_{def} t_1(g) \le t_2(g)\]

If one of the tactic is failing then $t_1$ and $t_2$ are not comparable through
this relation.
Given the default subsumption on goals, $t_1$ subsumes $t_2$ on $g$ if and only
if $t_1(g)$ is a subset (modulo $\alpha$-equivalence) of $t_2(g)$.
\end{definition}

In the end, the winning tactic of the orthogonalization competition can be
expressed by the formula:

\[\mathbb{O}(t,g) = \argmax_{x\ \in\
\mathit{Predictions}^{\mathit{tactic}}_k(g) \cup
\lbrace t
\rbrace} \lbrace
\textit{\mbox{Coverage}}(x)\
|\ x \le_g t\rbrace\]

All in all, a database build with the orthogonalization process contains
$(\mathbb{O}(t,g),g)$ instead of $(t,g)$.

\section{Abstraction}\label{sec:synthesis}
One of the major weakness of the previous version of \tactictoe was that
it could not create its own tactics. Indeed, sometimes no previous tactic is
adapted for a certain goal and creating a new tactic is necessary.
%Experiments in section, show how different degrees of tactic synthesis could
%improve the success rate of \tactictoe.
In this section we present a way to create tactics with different arguments
by abstracting them and re-instantiating them using a predictor for that kind
of argument. In the spirit of the orthogonalization method, we will try to
create tactics that are more general but have the same effect as the
ungeneralized one. The generalized tactics are typically slower that their
original variants, but the added flexibility is worthwhile in practice. Moreover,
since we impose a timeout on each tactic (0.05 seconds by default), very slow
tactics fail and thus are not selected.

\paragraph{Abstraction of Tactic Arguments}
The first step is to abstract arguments of tactics by replacing them by a
placeholder, creating a tactic with an unspecified argument.
Suppose we have a recorded tactic $t$. Since $t$ is a \sml code tree, we can
try to abstract any of the
\sml subterms. Let $u$ be a \sml subterm of $t$, and $h$ a placeholder in $u$, we
can substitute $u$ by $h$ to create an abstracted tactic.
We denote this intermediate tactic $t[h/u]$. By repeating the
process, multiple
arguments may be abstracted in the same tactic. Ultimately, the more
abstractions are performed the more general a tactic is, as many tactics
become instances of the same abstracted tactic. As a consequence, it becomes
harder and harder to predict suitable arguments. In our experiments, we create
one abstraction $\hat{t}$ for each tactic $t$ by abstracting all subterms of
type theorem list in $t$.

\paragraph{Instantiation of an Abstracted Tactic}
An abstracted tactic is not immediately applicable to a goal, since it contains
unspecified arguments. To apply an abstracted tactic $\hat{t}$, we first need
to
instantiate the placeholders inside $\hat{t}$. Because it is difficult to
build a general predictor effective on each type of argument, we manually
design different argument predictors for each type. Those predictors are
given a goal $g$ as input and return the best predicted argument for this
goal.

Our default algorithm relies on argument predictions for theorems with a
radius of 16. It uses the produced list
$\mathit{Predictions}^{\mathit{tactic}}_{16}(g)$ to replace all the
placeholders
in $\hat{t}$.
The type of theorem lists is a very common type in \holfour proofs and theorems contain
enough information to be predicted accurately.
We also have positive results with term abstraction and plan to include it soon
in our default algorithm.

\paragraph{Selection of Abstracted Tactics}
As abstracted tactics do not appear in human proofs, we need to find a way to
predict them so that they can contribute to the \tactictoe proof search.
A straightforward idea is to try $\hat{t}$ before $t$ during the proof search.
However, this risks doing unnecessary work as the two may perform similar steps.
Therefore, we would like to decide
beforehand if one is better than the other.
In fact, we can re-use the orthogonalization module to do this task for us.
We add $\hat{t}$ to the competition, initially giving it the coverage of $t$.
If $\hat{t}$
wins, it is associated with $g$ and is included in the database of tactic
features and thus can be predicted during proof search.
After many orthogonalization competitions, the coverage of $\hat{t}$ may exceed
the coverage of
$t$. At
this point, the coverage of $\hat{t}$ is estimated on its own and not inherited
from
$t$ anymore.




\section{Experimental Evaluation}\label{s:experiments}
\todo{Josef: give the specification}
Each re-proving experiment is performed on a single CPU of a server in Prague
with the exception of the experiments
relying on asynchronous \eprover calls that run on two CPUs.


\subsection{Methodology}
The evaluation imitates the construction of the library: For each theorem only
the previous human proofs are known. These are used as the learning base for
the predictions.
To achieve this scenario we re-prove all theorems during a modified build of
\holfour.
As theorems are proved, their tactical proofs and their statements are
recorded and included in the training examples.
For each theorem we first attempt to run the \tactictoe search with a time
limit of 60 seconds, before processing the original proof script.
In this way, the fairness of the experiments is guaranteed by construction.
Only previously declared \sml
variables (essentially tactics, theorems and simpsets) are accessible.
And for each theorem to be re-proven \tactictoe is only trained on previous
proofs.

\todoi{rephrase to something more coherent}
\paragraph{Datasets: optimization and validation}

All top-level theorems from the standard library are considered with the
exception of 440 hard problems (containing a \texttt{let} construction in their
proof) and 1175 easy problems (build from \texttt{save\_thm} calls).
Therefore, during the full-scale experiments, we evaluate 7164 theorems.
We use every tenth theorem of the first third of the standard library of
them for parameter optimization, which amounts to 273 theorems.
Although the evaluation of each set of parameters on its own is fair,
the selection of the best strategy in Section~\ref{sec:tuning} should also be
considered as a learning process. To ensure the global fairness, the final
experiments in Section~\ref{sec:full_exp} runs the best strategy on the full
dataset which is about 30 times larger.

\subsection{Tuning \tactictoe}~\label{sec:tuning}

Fix parameters: not optimized here.
 - Features extraction method.
 - k-nn distance.
 - MCTS evaluation radius is set to 10.
 - Metis : 16 lemmas.
 - Eprover: 128 lemmas and 5 seconds timeout 1 asynchronous call


\begin{table}[ht]
\centering\ra{1.3}
\small
\begin{tabular}{llcc}
\toprule
 Technique & Parameters & Solved (1s) & Solved (60s)\\
\midrule
Tactic time out & 0.02s && 154\\
                & 0.05s (default) && 156\\
                & 0.1s && 154\\
\midrule
Orthogonalization & none & 105 & 156 \\
                  & $radius = 10$ & 121 & 156 \\
                  & $radius = 20$ (default) & 121 & 156 \\
                  & $radius = 40$ & 124 & 156 \\
\midrule
Abstraction       & none (default)  && 156\\
                  & $theorems = 8$  && 195\\
                  & $theorems = 16$ && 199\\
                  & $theorems = 32$ && 195\\
\midrule
MCTS policy & $c_{policy} = 0.4$ && 149\\
            & $c_{policy} = 0.5$ (default) && 156\\
            & $c_{policy} = 0.6$ && 153\\
\midrule
MCTS evaluation & none (default + best abstraction) && 199\\
				& $c_{exploration} = 1$ && 201\\
				& $c_{exploration} = 2$ && 203\\
				& $c_{exploration} = 4$ && 198\\
\midrule
ATP integration & none (default + best abstraction && 203\\
                &  + best MCTS evaluation) &&\\
                & \metis 0.1s && 216\\
                & \metis 0.2s && 212\\
                & \metis 0.4s && 212\\
                & \eprover && 213\\
                & \metis 0.1s + \eprover && 218\\
\bottomrule
\end{tabular}
\caption{\label{tab:tuning} Number of problem solved with different set of
parameters for \tactictoe on a training set of 273 theorems.}
\end{table}

In Table~\ref{tab:tuning}, we run \tactictoe with different parameters.
The first four techniques are tested relative to the same baseline indicated in
the table by the tag ``default''.  This default strategy relies on a tactic
time out of 0.05s, an orthogonalization radius of 20 and a policy coefficent of
0.5.
The two last techniques are tested relative
to a baseline consisting of the best set of parameters discovered so far and
are marked with the improvement over the ``default'' strategy.
In each subtable, each experiment differs from the current baseline by exactly
one parameter.

Thanks to the larger search time available in this experiment, the timeout for
each tactic can be raised from the 0.02 seconds used in \tactictoe's first
experiment~\cite{tgckju-lpar17} to 0.05
seconds. Removing tactics with similar effect as performed by the
orthogonalization process is only beneficial when running \tactictoe for short
period of time. It seems that the alloted time allows a strategy
running without orthogonalization to catch up.
It is best to predict a list of 16 theorems to instantiate arguments of type
theorem list in tactics. At any rate, argument prediction for abstracted
tactics is the technique that have the highest impact on the success of
\tactictoe. Integration of ATPs being a specialization of this technique
contributes significantly as well. Experiments involving \eprover, run the ATP
on one thread asynchronously with a timeout of 5 seconds and 128 premises.
The fact that \metis outperforms \eprover in this setting is due to the fact
that \metis is run with a very short timeout, allowing to close different part
of search tree quickly. Because \metis is weaker that \tactictoe as an ATP, it
is given less predictions. But even if a lemma essential for the proof of a
goal is not predicted, a modification on the goal performed by a tactic can
change its prediction to include the necessary lemma.
This effect minimizes the drawback of relying on a small number of predictions.


The MCTS evaluation, despite relying on the largest amount of collected data,
does not provide a significant improvement over a strategy relying on no
evaluation. The minor improvement seen in the table is most likely due to some
variability in our experiments.
The main reason is that our predictor learning abilities is
limited and we are using the provability of lists of goals for evaluation. A
more accurate evaluation could be based on an estimation of the length of the
proof required to close a list of goals.



%\subsection{Increasing \tactictoe's knowledge}\label{sec:perfect_exp}
%\todoi{decide to keep or not}
%We investigate what are the limits of \tactictoe and what benefits additional
%knowledge could provide. To this end, we will modify the set of features to
%include features vectors that should not be known in a regular experiment. We
%include some vectors from the recorded human proof and perform the proof
%search
%after the recording instead of before in all other experiments. This
%experiment
%is not fair and should only be compared to
%experiments in the same settings.
%
%For clarity, we will name the set of feature vectors that can be recorded from
%the current human proof $\mathbb{H}$ and the set of vectors recorded from
%previous proofs $\mathbb{P}$ (consistent with previous notation?). We note
%$\pi_1^1$ the projection on the
%first component returning the label of a feature vector. And we define the
%function $T$ that takes a tactic and return the list of its tokens.
%
%We will progressively add bigger and bigger
%subset of $\mathbb{H}$ to $\mathbb{P}$ and observe the effect on the proof
%search success rates.
%
%Here are the list of increasing non-trivial subsets of $\mathbb{H}$ we
%consider:
%\begin{align*}
%\mathbb{F}_1 &= \lbrace (t,g) \in \mathbb{H}\ |\ t \in \pi_1^1(\mathbb{P})
%   \rbrace \\
%\mathbb{F}_2 &= \lbrace (t,g) \in \mathbb{H}\ |\ \forall x \in T(t).\ \exists
%t'\in \pi_1^1(\mathbb{P}).\ x \in T(t') \rbrace\\
%\mathbb{F}_3 &= \lbrace (t,g) \in \mathbb{H}\ |\ \forall x \in T(t).\ \exists
%t'\in \pi_1^1(\mathbb{P}).\ x \in T(t') \vee x\ \mbox{is a theorem} \rbrace\\
%\mathbb{F}_4 &= \lbrace (t,g) \in \mathbb{H}\ |\ \forall x \in T(t).\ \exists
%t'\in \pi_1^1(\mathbb{P}).\ x \in T(t') \vee x\ \mbox{is a theorem or a term}
%\rbrace\\
%\end{align*}
%
%
%$\mathbb{F}_1$ is an approximation of what we could get if we had perfect
%prediction for those tactics.
%
%\begin{table}[h]
%\centering\ra{1.3}
%\small
%\begin{tabular}{lll}
%\toprule
% & Additional features & Solved \\
%\midrule
%$\emptyset$   & none & 119 (43.6\%) \\ %NONEv2
%$\mathbb{F}_1$& tactic prediction & 121 (44.3\%) \\ %AFTERTACv2
%$\mathbb{F}_2$& token recombination  & 137 (50.2\%) \\ %AFTERTOKENv2
%$\mathbb{F}_3$& theorem prediction  & 207 (75.8\%) \\ %AFTERTHMTHMv2
%$\mathbb{F}_4$& term prediction & 223 (81.7\%) \\ %AFTERALLv2
%$\mathbb{H}$  & all & 239 (87.5\%) \\ %AFTERSMALLv2
%% $AFTERv2$   & $\mathfrak{H}$ & 245 (89.7\%)  \\
%\bottomrule
%\end{tabular}
%\caption{\label{tab:featue_param} Effect of augmenting \tactictoe's knowledge
%with tactics from the proof of the tested theorem. The experiments was run
%with
%a time out of 5 seconds on an older version of \tactictoe, as can be seen in
%the low success rate of thedefault strategy.}
%\end{table}
%
%
%
%And we observe in Table~\ref{tab:featue_param} a
%slight increase compared to the default \tactictoe. We then included all
%vectors that contains tactics that can be rebuild from tokens of
%tactics in our database.
%Our search algorithm proved two proofs less that if it had perfect prediction
%so it means that our prediction algorithm (policy) is doing a good  job at
%least in this early settings where proofs are easy.
%Suprisingly, being able to recombine tokens of tactics from the database is
%barely sufficient to prove more than half of the theorems in this dataset. The
%Therefore, additionally including in tactics theorems and terms that did not
%appear in previous tactics is essential. Creating from scratch other arguments
%may not help as much. This is why we restricted ourselves to predicting
%arguments of theorem list type and the term type during tactic synthesis.
%Finally our proof search algorithm sometimes fails even if it knows all the
%tactic necessary. The two main reasons are that either a crucial tactic
%exceeds its timeout or that the human proof is too deep to be rediscovered
%(more than 10 steps long).In all those experiments orthogonalization is
%performed with
%radius 20.


\subsection{Full-scale experiment}~\label{sec:full_exp}

Based on the results of parameter tuning, we now evaluate a version of
\tactictoe with its best parameters on an evaluation set of 7164 theorems from
the \holfour standard library. We compare it with the performance of \eprover.
For reasons of fairness, \eprover asynchronous calls are not included in the
best \tactictoe strategy. The ATP \eprover is run in auto-schedule mode with
128 premises. The settings for \tactictoe are the following:
0.05 seconds tactic timeout, an orthogonalization radius of 20, theorem list
abstraction with 16 predicted theorems for instantiation, a prior policy
coefficient of 0.5, an evaluation radius of 10, an exploration coefficient of 2
and priority calls to \metis with a timeout of 0.1 seconds.

\begin{table}[h!]
\centering\ra{1.3}
\small
\begin{tabular}{lc}
\toprule
  & Solved (60s) \\
\midrule
   \eprover   & 2472 (34.5\%)\\
   \tactictoe & 4760 (66.4\%)\\
\midrule
   Total  & 4946 (69.0\%)\\
\bottomrule
\end{tabular}
\caption{Evaluation on 7164 top-level theorems of the \holfour standard library
\label{tab:_param}}
\end{table}

The results shows that \tactictoe is able to prove almost twice as much
theorems as \eprover. Combining the results of \tactictoe and \eprover we get a
69.0\% success rate which is significantly above the 5O\% success rates
of hammers on this type of problems~\cite{tgck-cpp15}. Moreover, \tactictoe is
running a single set of parameters (strategy), where as hammers and ATPs have
been optimized and rely on a wide range of strategies.


\paragraph{Reconstruction}
Tactic proofs produced by \tactictoe during this experiment are all verifiable
in \holfour. By the design of the proof search, reconstruction of \tactictoe
proof succeeds, unless one of the tactic modifies the state of \holfour in a
way that changes the behavior of a tactic used in the final proof. Such an
event has not yet occurred.
More details on how a proof is extracted from the search tree is given in
Section~\ref{sec:proofdisplay}.
For \eprover, reconstruction rate varies between 95 and 99 percent, depending
on the reconstruction method \cite{}\todo{CK}.

\begin{table}[]
\centering
\setlength{\tabcolsep}{3mm}
\begin{tabular}{@{}ccccc@{}}
\toprule
\phantom{ab} & {arith} & {real} & {compl} & {meas} \\
\midrule
\tactictoe & 81.2 & 74.0 & 79.6 & 31.3\\
\eprover & 59.9 & 72.0 & 67.1 & 12.8\\
\midrule
\phantom{abc} & {proba} & {list} & {sort} & {f\_map} \\
\midrule
\tactictoe & 45.8 & 79.5 & 65.3 & 82.0 \\
\eprover & 24.1 & 26.5 & 15.8 & 24.7 \\
\bottomrule
\end{tabular}
\caption{\label{theories}Percentage (\%) of re-proved theorems in the theories
\texttt{arithmetic}, \texttt{real}, \texttt{complex}, \texttt{measure},
\texttt{probability}, \texttt{list}, \texttt{sorting} and \texttt{finite\_map}.
}
\end{table}

Table~\ref{theories} compares the re-proving success rates for different
\holfour theories. \tactictoe outperforms \eprover on every
considered theory.
\eprover is more suited to deal with dense theories such as
\texttt{real} or \texttt{complex} where a lot of related theorems are available
and most proofs are usually completed by rewriting tactics. Thanks to its
ability to re-use custom-built tactics, \tactictoe
largely surpasses \eprover on theories relying on inductive terms and
simplification sets such as \texttt{arithmetic}, \texttt{list}
and \texttt{f\_map}. Indeed, \tactictoe is able to recognize where and when to
apply induction, a task at which ATPs are known to struggle with.

%\subsection{Recent improvements}\todo{maybe remove this subsection}
%The final results could be improved by a few percent by including the
%following additional techniques that are already available.
%The technique already discussed in Section~ would most likely improve the
%results by 2-3 percent. Reducing the exploration coefficient from 4 to 2 would
%also probably give a 3 percent increase in the number of proven theorems as
%shown by a recent 1 in 10 evaluation on the full library.

\paragraph{Self-learning}\todo{explain better or remove or move to future work}
All our feature vectors have been learned form human proofs. We now can now
also add tactic-goal pairs that appears in final \tactictoe's proof. To prevent
duplication of effort, orthogonalization of those
tactics is essential to have a beneficial effect.
Since recording and re-proving are intertwined, the additional data is
available for the next proof search.
The hope is that the algorithm will improve faster by learning from its own
discovered proofs than from the human proof
scripts~\cite{DBLP:conf/cade/Urban07}. Side experiments indicate that
self-learning improves \tactictoe's success rate by less than one percent.



\subsection{Complexity of the proof search}

\pgfplotscreateplotcyclelist{my black}{
solid, mark repeat=100, mark phase=0, black!100\\
dashed, mark repeat=100, mark phase=0, black!100\\
}



\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1]
\begin{axis}[
  legend style={anchor=south east, at={(0.9,0.1)}},
  width=\textwidth,
  height=0.7*\textwidth,xmin=0, xmax=60,
  ymin=0, ymax=4800,
  xtick={},
  ytick={},
  cycle list name=my black]
\addplot table[x=time, y=solved] {data/tactictoe_time};
\addplot table[x=time, y=solved] {data/eprover_time};
\legend{\tactictoe,\eprover}
\end{axis}
\end{tikzpicture}
\caption{\label{fig:c1} Number of problems solved in less than x seconds.}
\end{figure}

We first investigate how \tactictoe and \eprover scale as the search time
grows in Figure~\ref{fig:c1}.
In 10 seconds, \tactictoe solves 90 percent of the problems it can
solve in 60 seconds. The analysis is a bit different for \eprover. Indeed,
we can clearly deduce from the bump at 30 seconds that \eprover is using at
least two strategies and probably more. Strategies are a useful method to
fight the exponential decline in the number of newly proven theorems over
time. Therefore, integrating strategy scheduling in \tactictoe could be
something to be experimented with. Overall, we recommend using \tactictoe and
\eprover with a 10 seconds timeout. It is possible also to run them with a
longer time limit, but we advise to do so in parallel to avoid waiting time.
\todo{its strange to ``advise'' in a paper?}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1]
\begin{axis}[
  legend style={anchor=north east, at={(0.9,0.9)}},
  width=0.5*\textwidth,
  height=0.5*\textwidth,
  ymin=0, ymax=3000,
  xmin=0, xmax=10,
  xtick={},
  ytick={},
  cycle list name=my black]
\addplot table[x=length, y=proofs] {data/tactictoe_proof_length};
\addplot table[x=length, y=proofs] {data/original_proof_length};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}[scale=1]
\begin{axis}[
  legend style={anchor=north east, at={(0.9,0.9)}},
  width=0.5*\textwidth,
  height=0.5*\textwidth,
  ymin=0, ymax=120,
  xmin=10, xmax=50,
  xtick={},
  ytick={},
  cycle list name=my black]
\addplot table[x=length, y=proofs] {data/tactictoe_proof_length};
\addplot table[x=length, y=proofs] {data/original_proof_length};
\legend{\tactictoe proofs,human proofs}
\end{axis}
\end{tikzpicture}
\caption{\label{fig:distrib}Number of proof scripts with $x$ tactic units.}
\end{figure}

In order to appreciate the difficulty of the evaluation set from a human
perspective, the length distribution of human proofs in the
validation set is shown in Figure~\ref{fig:distrib}.  It is clear from the
graph that most of the human proofs are short. The proofs found by \tactictoe
follows a similar
distribution. This means that if \tactictoe finds a proof, there is about a 50
percent chance that it will be shorter than what a human would come up with.


\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1]
\begin{axis}[
  legend style={anchor=north east, at={(0.9,0.9)}},
  width=\textwidth,
  height=0.7*\textwidth,xmin=0, xmax=20,
  ymin=0, ymax=100,
  xtick={},
  ytick={},
  cycle list name=my black]
\addplot table[x=oplen, y=solved] {data/tactictoe_by_oplen};
\addplot table[x=oplen, y=solved] {data/eprover_by_oplen};
\legend{\tactictoe,\eprover}
\end{axis}
\end{tikzpicture}
\caption{\label{fig:percentage}Percentage of problem solved with respect to the
length of the
original proof until length 20.}
\end{figure}

In Figure~\ref{fig:percentage}, we regroup the human proofs by their length to
see how well \tactictoe and \eprover performs on increased level of
difficulties. As expected, the longer the original proof is, the harder it is
for \tactictoe and \eprover to re-prove the theorem on their own.
The performance of \tactictoe is compelling with more than half of the theorems
that required a proof of length six being reproven. It is also consistently
better than \eprover for any proof length.


\section{Proof Recording}\label{sec:recording}
Recording proofs in an LCF-style proof assistant can be done at different
levels.
In \holfour all existing approaches relied on modifying the kernel. This was
used
either to export the primitive inference
steps~\cite{Wong95recordingand,DBLP:conf/itp/KumarH12}
or to record dependencies of theorems~\cite{tgck-cpp15}. This was not suitable
for our
purpose of learning proving strategies at the intermediate tactic level. We
therefore
discuss recording proofs in an LCF-style proof assistant, with the focus on
\holfour
in this section.
%
Rather than relying on the underlying programming language, we parse the
original theory file containing proof scripts. This enables us to extract the
code of each tactic.
Working with the string representation of a tactic is better than working with
its value:
\begin{itemize}
\item Equality between two functions is easy to compute from
their string representation, which allows us to avoid
predicting the same tactic repeatedly.
\item
The
results of
\tactictoe can be returned in a readable form to the user. In this way, the
user can learn from the feedback, analyze the proof and possibly improve on it.
Furthermore \tactictoe does not need to be installed to run
proof scripts generated by \tactictoe. In this way, the further development of
\tactictoe does not affect the robustness of \holfour.
\item
It is difficult (probably impossible) to transfer \sml values
between theories, since they are not run in the same \holfour session. In contrast,
\sml code can be exported and imported.
\item To produce a tactic value form a code is easy in \sml, which can be
achieved by using a reference to a tactic and update it with the function
\texttt{use}.
\end{itemize}

In order to transfer our tactic knowledge between theories, we want to be able
to re-use the code of a tactic recorded in one theory in an
other. We also would like to make sure that the code is interpretable and that
its interpretation does not change in the other theory.

Even when developing one theory, the context is continually changing:
modules are opened and local identifiers are defined. Therefore, it is unlikely
that code extracted from proof scripts is interpretable in any other part of
\holfour without any post-processing.
To solve this issue, we recursively replace each local identifier by its
definition until we are able to write any expression with global identifiers
only. Similarly, we prefix each global identifier by its module.
We call this process \emph{globalization}. The result is a standalone \sml code
executable in any \holfour theory.  In the absence of side effects, it is
interpreted in the same way across theories.
Therefore, we can guarantee that the behavior of recorded stateless tactics
does not change. And because an update on a stateful tactic
generally increases its strength, our prediction algorithm based on past
examples is still reasonably effective in this case.

\subsection{Implementation}
We describe in more detail our implementation of the recording algorithm. It
consists of 4 phases: proof script extraction, proof script globalization,
tactic unit wrapping, and creation of tactic-goal pairs.

Because of the large number of \sml constructions, we only describe the effect
of these steps on a running example that contains selected parts of the theory
of lists.

%We comment on the treatment of global and local identifiers but  omit more
%involved constructions
%such as the globalization of infix operators.

\begin{example}\label{ex:running}(Running example)
\small
\begin{lstlisting}[language=SMLSmall]
open boolLib Tactic Prim_rec Rewrite
ldots
val LIST_INDUCT_TAC = INDUCT_THEN list_INDUCT ASSUME_TAC
ldots
val MAP_APPEND = store_thm("MAP_APPEND",
  ``!(f:'a->'b) l1 l2. MAP f (APPEND l1 l2) = APPEND (MAP f l1) (MAP f l2)``,
  STRIP_TAC THEN LIST_INDUCT_TAC THEN ASM_REWRITE_TAC [MAP, APPEND])
\end{lstlisting}
\end{example}

The first line of this script file opens modules (called structures in \sml). Each of
the modules contains a list of global identifiers which become directly accessible in
the rest of the script.
A local identifier \texttt{LIST\_INDUCT\_TAC} is declared next, which is a
tactic that performs induction on lists. Below that, the theorem
\texttt{MAP\_APPEND} is proven.
The global tactic \texttt{STRIP\_TAC} first removes universal quantifiers. Then,
the goal is split into a base case and an inductive case. Finally, both
of these cases are solved by \texttt{ASM\_REWRITE\_TAC [MAP, APPEND]}, which
rewrites assumptions with the help of theorems previously declared in this
theory.

We first parse the script file to extract tactic proofs. Each of them is found in
the third argument of a call to \texttt{store\_thm}. The result of tactic proof
extraction for the running example in presented in Example~\ref{ex:r0}.

\begin{example}\label{ex:r0}(Tactic proof extraction)
\small
\begin{lstlisting}[language=SMLSmall]
STRIP_TAC THEN LIST_INDUCT_TAC THEN ASM_REWRITE_TAC [MAP, APPEND]
\end{lstlisting}
\end{example}

In the next phase, we globalize identifiers of the tactic proofs.
Infix operators such as \texttt{THEN} need to be processed in a special way so
that they keep their infixity status after globalization. For simplicity,
the
globalization of infix operators is omitted in Example~\ref{ex:r1}.
In this example, the three main cases that can happen during the globalization
are depicted. The first one is the globalization of identifiers declared in
modules. The global identifiers \texttt{STRIP\_TAC} and
\texttt{ASM\_REWRITE\_TAC} are prefixed by their module \texttt{Tactic}. In
this way, they will still be interpretable whether \texttt{Tactic} was open or
not. The local identifier \texttt{LIST\_INDUCT\_TAC} is replaced by its
definition which happens to contain two global identifiers.
The previous paragraph describes the globalization for all identifiers except
local theorems.  We do not replace a local
theorem by its definition because we want to avoid unfolding tactic proofs into
other tactic proofs.
If the theorem is stored in the \holfour database available across \holfour
developments, we can obtain the theorem value by calling \texttt{DB.fetch}.
Otherwise the globalization process fails and the local theorem identifier is
kept unchanged. A recorded tactic with an unchanged local theorem as argument
is only interpretable inside the current theory.

\begin{example}\label{ex:r1} (Globalization)
\begin{lstlisting}[language=SMLSmall]
Tactic.STRIP_TAC THEN
Prim_rec.INDUCT_THEN (DB.fetch "list" "list_INDUCT") Tactic.ASSUME_TAC THEN
Rewrite.ASM_REWRITE_TAC [DB.fetch "list" "MAP", DB.fetch "list" "APPEND"]
\end{lstlisting}
\end{example}

Running the globalized version of a tactic proof will have the exact same
effect as the original. But since we want to extract information from this
script in the form tactics and their input goals, we modify it further.
In particular, we need to define at which level we should record the tactics in
the tactic proof. The simplest idea would be to record all \sml subexpressions
of type \texttt{tactic}. However, it will damage the quality of our data by
dramatically increasing the number of tactics associated with a single goal.
Imagine a proof script of the form \texttt{A THEN B THEN C}; then the tactics
\texttt{A}, \texttt{A THEN B} and \texttt{A THEN B THEN C} would be valid
advice for something close to their common input goal. The tactic
\texttt{A THEN B THEN C} is likely to be specific. In contrast, we can consider
the tactic \texttt{REPEAT A} that is repetitively calling the tactic \texttt{A}
until
\texttt{A} has no effect. In this situation, it is often preferable to record
only \texttt{REPEAT A} than calls to \texttt{A}. Otherwise during the
proof search, we would have to make a branches created after each call of
\texttt{A}.
To sum up, we would like to record only the most general tactics which
make the most progress on a goal. As a trade-off between these two objectives,
we split proofs into tactic units.

\begin{definition}(Tactic unit)
A tactic unit is an \sml expression of type \texttt{tactic} that does not contain a
infix operator at its root.
\end{definition}

Because these tactic units are constructed from visual information present in
the tactic proof, they often represent what a human consider to be one step of
the proof.

To produce the final recording script, we encapsulate each tactic unit
in a recording function \texttt{R} (see Example~\ref{ex:wrap}). In order to
record
tacitcs in all \holfour theories, we replace the original proof by the
recording script in each theory and rebuild the \holfour library.

\begin{example}\label{ex:wrap} (Tactic unit wrapping)
\begin{lstlisting}[language=SMLSmall]
R "Tactic.STRIP_TAC" THEN
R "Prim_rec.INDUCT_THEN (DB.fetch \"list\" \"INDUCT\") Tactic.ASSUME_TAC" THEN
R "Rewrite.ASM_REWRITE_TAC
  [fff \"list\" \"MAP\", fff \"list\" \"APPEND\"]"
\end{lstlisting}
\end{example}

At run time the function \texttt{R} is designed to observe what input goals a
tactic  receives without changing its output. The implementation of \texttt{R}
is presented in Example~\ref{ex:record}.

\begin{example}\label{ex:record} (Code of the recording function)
\begin{lstlisting}[language=SMLSmall]
fun R stac goal = (save (stac,goal); tactic_of_sml stac goal)
\end{lstlisting}
\end{example}

The function \texttt{save} writes the tactic-goal pair to disk increasing the
number of entries in our database of tactics. The function
\texttt{tactic\_of\_sml} interprets the \sml code \texttt{stac}. The tactic is
then applied to the input goal to replicate the original behavior.
After all modified theories are rebuild, each call to a wrapped tactic in a
tactic proof is recorded as a pair containing its globalized code and its
input goal.



\section{Proof Presentation}\label{sec:proofdisplay}

When \tactictoe finds a proof of a goal $g_{root}$, it returns a search tree
$\mathfrak{T}$ where the root node containing $g_{root}$ is solved.
In order to transform this search tree into a proof script, we need to extract
the tactics that contributed to the proof and combine them using tactic
combinators.

By the design of the search, a single tactic combinator, \texttt{THENL}, is
sufficient. It combines a tactic $t$ with a list of subsequent ones, in such a
way that after $t$ is called, for each created goal a respective
tactic from the list is called.

Let $T_{sol}$ be a partial function that from a goal $g$ returns a tactic $t$
for which $A(g,t)$ is solved in $\mathfrak{T}$.
The proof extraction mechanism is defined by mutual
recursion on goals and nodes of $\mathfrak{T}$ by the respective function
$P_{goal}$ and $P_{node}$:

\begin{align*}
P_{goal}(g) &=_{def} T_{sol}(g)\ \texttt{THENL}\ P_{node}(A(g,T_{sol}(g)))\\
P_{node}(a) &=_{def} [P_{goal}(g_1),\ldots,P_{goal}(g_n)]\ \ \ \text{with}\
G(a) = g_1,\ldots,g_n\\
\end{align*}

The extracted proof script of $\mathfrak{T}$ is $P_{goal}(g_{root})$.
We minimally improve it by subtituting \texttt{THENL} by \texttt{THEN} when the
list of goals is a singleton and removing \texttt{THENL []} during the
extraction phase.
Further post-processing such as
eliminating unnecessary tactics and theorems has been developed and
improve the user experience greatly~\cite{DBLP:conf/sefm/Adams15}.

\paragraph{Minimizing length of the proof}
A fast and simple minimization is applied when processing the final proof. If
the tactic \texttt{A THEN B} appears in the proof and has the same effect as
\texttt{B} then we can replace \texttt{A THEN B} by \texttt{B} in the script.
Optionally, stronger minimization can be obtained
by rerunning \tactictoe with a low prior
policy coefficient, no prior evaluation and a database of tactics that contains
tactics from the discovered proof only.

\paragraph{Minimizing tactic arguments}
Let $t$ be a tactic applied to a goal $g$ containing a list $l$ (of theorems)
as argument. And let $t'$ be the tactic $t$ where one element $e$ of $l$ as be
removed. If $t$ and $t'$ have the same effect on $g$ then $t'$ can replace $t$
in the final proof. This process is repeated for each element of $l$.
This is a generalization of the simplest method used for minimizing a list of
theorems in ``hammers'' \cite{hammers4qed}.

\paragraph{Prettification}
Without prettification, the returned tactic is barely readable as it contains
information to guarantees that each \sml subterm is interpreted in the way in
any context. Since we return the proof at a point where the \sml interpreter is
a specific state, we can strip unnecessary information in the form of
module prefixes. If possible, we group all local declarations in the
script under a single \texttt{let} binding at the start of the script. And we
replace extended terms by their quoted version.
All in all, if a prettified tactic $t_p$ has the same effect its original
$t_o$, we replace $t_p$ by $t_o$ in the proof.\\

The total effect on the readability of a proof script is depicted in
Example~\ref{ex:pretty}.
\todo{run without prettification}
\begin{example}\label{ex:pretty}\ \\
Discovered proof script for the theorem \texttt{EVERY\_MAP}
\begin{lstlisting}[language=SMLSmall]
boolLib.REWRITE_TAC [DB.fetch "list" "EVERY_CONJ", DB.fetch "list" "EVERY_MEM",
  DB.fetch "list" "EVERY_EL", ldots ,  combinTheory.o_DEF] THEN
BasicProvers.Induct_on [HolKernel.QUOTE " (*#loc 1 11380*)l"] THENL
  [BasicProvers.SRW_TAC [] [],
   simpLib.ASM_SIMP_TAC (BasicProvers.srw_ss ()) [boolLib.DISJ_IMP_THM,
     DB.fetch "list" "MAP", DB.fetch "list" "CONS_11", boolLib.FORALL_AND_THM]]

\end{lstlisting}
Minimized and prettified proof script:
\begin{lstlisting}[language=SMLSmall]
Induct_on `l` THENL
  [SRW_TAC [] [], ASM_SIMP_TAC (srw_ss ()) [DISJ_IMP_THM, FORALL_AND_THM]]
\end{lstlisting}
\end{example}

\section{Case Study}\todo{comment on the case study}
Since the proofs generated by \tactictoe are meant to be part of a \holfour development, it interesting to compare their quality with the original human proofs.
The quality of a proof in \holfour can be measured in terms of length, readability, maintainability and verification speed.
We study these properties in three examples taken from our full-scale experiment.
The time needed by \holfour to check a proof is shown in parentheses.

Example~\ref{ex:cs1} is about proving that greatest common divisors are unique.
The human developer recognizes that it follows easily from two theorems.
Our prover \tactictoe is not able to find that \texttt{DIVIDES\_ANTISYM} is
necessary directly from the shape of the original theorems so it rewrites the
goal then splits the goal into multiple cases until the goal obligations are similar
to \texttt{DIVIDES\_ANTISYM} to be predicted. As expected, \tactictoe's proof
also takes much longer to check.

\begin{example}\label{ex:cs1} \texttt{IS\_GCD\_UNIQUE} in theory \texttt{gcd}
\begin{lstlisting}[language=SMLSmall]
!a b c d. is_gcd a b c wedge is_gcd a b d ==> (c = d)
\end{lstlisting}
Human proof (5 milliseconds)
\begin{lstlisting}[language=SMLSmall]
PROVE_TAC[IS_GCD, DIVIDES_ANTISYM]
\end{lstlisting}
\tactictoe proof (80 milliseconds)
\begin{lstlisting}[language=SMLSmall]
STRIP_TAC THEN
REWRITE_TAC [fetch "gcd" "is_gcd_def"] THEN
REPEAT Cases THENL
  [METIS_TAC [],
   REWRITE_TAC [SUC_NOT, ALL_DIVIDES_0, compute_divides] THEN
     METIS_TAC [NOT_SUC],
   METIS_TAC [NOT_SUC, DIVIDES_ANTISYM],
   METIS_TAC [LESS_EQUAL_ANTISYM, DIVIDES_LE, LESS_0],
   METIS_TAC [],
   RW_TAC numLib.arith_ss [divides_def],
   METIS_TAC [DIVIDES_ANTISYM],
   METIS_TAC [LESS_EQUAL_ANTISYM, DIVIDES_LE, LESS_0]]
\end{lstlisting}
\end{example}

In Example~\ref{ex:cs2}, we try to prove that for any surjective function $f$
from $s$ to $t$, there exists an injective function $g$ from $t$ to $s$ such
that $f\ o\ g$ is the identity function. The human proof is quite complicated.
In contrast, \tactictoe finds a much smaller proof that expands the
definition of injectivity and surjectivity and calls \metis.
However, the \tactictoe's proof takes much longer to check due to the proof search happening inside \metis.

\begin{example}\label{ex:cs2} \texttt{SURJ\_INJ\_INV} in theory
\texttt{pred\_set}
\begin{lstlisting}[language=SMLSmall]
!f s t. SURJ f s t ==> ?g. INJ g t s wedge !y. y IN t ==> (f (g y) = y)
\end{lstlisting}
Human proof (2 milliseconds)
\begin{lstlisting}[language=SMLSmall]
REWRITE_TAC [IMAGE_SURJ] THEN
DISCH_TAC THEN Q.EXISTS_TAC `THE o LINV_OPT f s` THEN
BasicProvers.VAR_EQ_TAC THEN REPEAT STRIP_TAC THENL
  [irule INJ_COMPOSE THEN Q.EXISTS_TAC `IMAGE SOME s` THEN
     REWRITE_TAC [INJ_LINV_OPT_IMAGE] THEN REWRITE_TAC [INJ_DEF, IN_IMAGE] THEN
     REPEAT STRIP_TAC THEN REPEAT BasicProvers.VAR_EQ_TAC THEN
     FULL_SIMP_TAC std_ss [THE_DEF],
   ASM_REWRITE_TAC [LINV_OPT_def, o_THM, THE_DEF] THEN
     RULE_ASSUM_TAC (Ho_Rewrite.REWRITE_RULE
       [IN_IMAGE', GSYM SELECT_THM, BETA_THM]) THEN ASM_REWRITE_TAC []]
\end{lstlisting}
\tactictoe proof (50 milliseconds)
\begin{lstlisting}[language=SMLSmall]
SRW_TAC [] [SURJ_DEF, INJ_DEF] THEN METIS_TAC []
\end{lstlisting}
\end{example}

In Example~\ref{ex:cs3}, we prove a theorem about lists, a domain where \tactictoe excels compared to ATPs.
Given two list $l_1$ and $l_2$ where the length of $l_1$ (noted $p$) is less
than a natural
number $n$, the theorem states updating the $n^{th}$
element of the concatenation of $l_1$ and $l_2$ is the same as
updating the $m^{th}$ element of $l_2$ where $m = n - p$. Again,
\tactictoe's proof is
much more readable starting with applying induction on $l$. It solves the base
case of the induction by rewriting and distinguish cases where $n=0$ or $n>0$
in the inductive hypothesis. It finalizes the proof with a short call to \metis
using the definition of
\texttt{LUPDATE}. Here, \tactictoe's proof is arguably smaller, faster and
easier to
understand and maintain. Such proofs after an expert review could replace the
their respective original human proofs in the \holfour repository.

\begin{example}\label{ex:cs3} \texttt{LUPDATE\_APPEND2} in theory
\texttt{rich\_list}
\begin{lstlisting}[language=SMLSmall]
!l1 l2 n x. LENGTH l1 <= n ==>
  (LUPDATE x n (l1 ++ l2) = l1 ++ (LUPDATE x (n - LENGTH l1) l2))
\end{lstlisting}
Human proof (63 milliseconds)
\begin{lstlisting}[language=SMLSmall]
  rw[] THEN simp[LIST_EQ_REWRITE] THEN Q.X_GEN_TAC `z` THEN
  simp[EL_LUPDATE] THEN rw[] THEN simp[EL_APPEND2,EL_LUPDATE] THEN
  fs[] THEN Cases_on `z < LENGTH l1` THEN
  fs[] THEN simp[EL_APPEND1,EL_APPEND2,EL_LUPDATE]
\end{lstlisting}
\tactictoe proof (17 milliseconds)
\begin{lstlisting}[language=SMLSmall]
Induct_on `l1` THENL [SRW_TAC [] [],
  Cases_on `n` THENL [SRW_TAC [] [],
    FULL_SIMP_TAC (srw_ss ()) [] THEN METIS_TAC [LUPDATE_def]]]
\end{lstlisting}
\end{example}

In Example~\ref{ex:cs4}, we demonstrate the capabilities of \tactictoe on a
goal that was not already proved in the \holfour libraries: that the set of
numbers $\{0,...,n+m-1\} \setminus \{0,...,n-1\}$ is the same as the set
obtained by adding $n$ to everything in $\{0,...,m-1\}$. In this example,
\tactictoe uses the simplification set \texttt{ARITH\_ss} to reduce arithmetic
formulas. This exemplifies an another advantage that \tactictoe has over ATPs,
mainly its ability to take advantage of user-defined simplification sets.

\begin{example}\label{ex:cs4}
\begin{lstlisting}[language=SMLSmall]
count (n+m) DIFF count n = IMAGE ((+) n) (count m)
SRW_TAC [ARITH_ss] [EXTENSION, EQ_IMP_THM] THEN
Q.EXISTS_TAC `x - n` THEN
SRW_TAC [ARITH_ss] []
\end{lstlisting}
\end{example}


\section{Related Work}
There are several essential components of our work that are comparable to
previous approaches: tactic-level proof recording, tactic
selection through machine learning techniques and automatic tactic-based proof
search. Our work is also related to previous approaches that use machine
learning to select premises for the ATP systems and guide ATP proof search
internally.

For \hollight, the Tactician tool~\cite{DBLP:conf/sefm/Adams15}
can transform a packed tactical proof into a series of interactive tactic
calls. Its principal application
was so far refactoring the library and teaching common proof techniques to new
ITP users. In our work, the splitting of a proof into a sequence of tactics is
essential for the
tactic recording procedure, used to train our tactic prediction mechanism.

The system
\textsf{ML4PG}~\cite{DBLP:journals/corr/abs-1212-3618,DBLP:journals/mics/HerasK14}
groups related proofs thanks to its clustering
algorithms. It allows \coq users to inspire themselves from similar proofs and
notice
duplicated proofs. Comparatively, our predictions comes from a much more
detailed description of the targeted goal.
%However, we simply create a single label for each tactic call whereas each of
%its
%arguments is treated independently in \textsf{ML4PG}.
%Our choice is motivated by the k-NN algorithm already used in
%\holyhammer for the a of theorems.
%, which can be easly adapted to the selection of tactics but would be improved
%by a
%version that can learn to predict tactics and their arguments independently.

\textsf{SEPIA}~\cite{DBLP:conf/cade/GransdenWR15} is a powerful system able to
generate
proof scripts from previous \coq proof examples.
Its strength lies in its ability to produce likely sequences
of tactics for solving domain specific goals. It operates by creating a model
for common sequences of tactics for a specific library.
This means that in order to propose the following tactic, only the previously
called tactics
are considered.
%intuition tells us that it should not be the main factor
Our algorithm, on the other hand, relies mainly on the characteristics of the
current goal
to decide
which tactics to apply next. In this way, our learning mechanism has to
rediscover why each
tactic was applied for the current subgoals. It may lack some useful bias for
common sequences
of tactics, but is more reactive to subtle changes. Indeed, it can be trained
on a large library and only tactics relevant to the current subgoal will be
selected.
Concerning the proof search, \textsf{SEPIA}'s %simple
breadth-first search is replaced by MCTS which allows for supervised learning
guidance in the exploration of the search tree.
Finally, \textsf{SEPIA} was evaluated on three chosen parts of the
\coq library demonstrating that it globally outperforms individual \coq
tactics. Here, we demonstrate the competitiveness of our system against
\eprover on the \holfour standard library.
%A recent improvement on this system~\cite{}\todo{Cezary: could not remember
%the
%reference that you gave me here i think it was CICM 2017} includes the
%possibility to
%substitute theorems appearing in tactics by ones belonging to the same
%cluster.
%In this work, we are also able to predict theorems appearing as arguments of
%tactics but we rely only on information from the current goal. Combining the
%two approaches by predicting from the goal and from the original tactic
%arguments is more difficult but is likely to produce more suitable lemmas.
%Proof patching: \cite{RingerYLG18}

A similar effort for stronger automation is performed in \isabelle. Nagashima
developed a proof strategy language PSL~\cite{NagashimaK17psl} which allows the
user to
program with regular tactics and combine them with tools such as
\sledgehammer~\cite{sledgehammer10} or Nitpick~\cite{Nitpick10}.
Selection of tactics from data collected in recorded proofs is in development
but for now it is up to the user to come up with a good meta-tactic.


Machine learning has also been used to advise the best library lemmas for new
ITP goals.
This can be done either in an interactive way, when the user completes the
proof based on the recommended lemmas, as in the \textsc{Mizar Proof
Advisor}~\cite{Urb04-MPTP0}, or attempted fully automatically, where such lemma
selection is handed over to the ATP component of a \emph{hammer}
system~\cite{hammers4qed,tgck-cpp15,holyhammer,BlanchetteGKKU16,mizAR40}.

Internal learning-based selection of tactical steps inside an ITP is analogous
to internal learning-based selection of clausal steps inside ATPs such as
\textsc{MaLeCoP}~\cite{malecop} and \textsc{FEMaLeCoP}~\cite{femalecop}. These
systems
use the naive Bayes classifier to  select clauses for the extension steps in
tableaux proof search based on many previous proofs. Satallax~\cite{Brown2012a}
can guide its
search internally~\cite{mllax} using a command classifier, which can estimate
the priority of the 11 kinds of
commands in the priority queue based on positive and negative examples.

\section{Conclusion}\label{sec:concl}
We proposed a new proof assistant automation technique which combines
tactic-based proof search with machine learning prediction
Its implementation, \tactictoe, achieves an overall success rate of 66.4\%
on 7164 theorems of the \holfour standard library, surpassing \eprover
with auto-schedule. Its
effectiveness is especially visible on
theories which use inductive data structures, specialized decision procedures,
and custom built simplification sets.
Thanks to the learning abilities of \tactictoe, the generated proof scripts
usually reveal the high-level structure of the proof. %For these reasons,
We therefore believe that predicting ITP tactics based on the current goal
features is a very reasonable approach to automatically guiding proof search,
and that accurate predictions can be obtained by learning from the knowledge
available in today's large formal proof corpora.

To improve the quality of the predicted tactics,
we would like to predict other type of arguments independently, as it was done
for theorems. In this direction, the most interesting arguments to
predict next are terms as they are ubiquitous in tactics. Further along the way,
new tactics could be created by programming them with any construction
available in \sml. There is also something to be gain by
having a better guidance for the search.
That is why se also want to experiment with stronger machine learning algorithm
such as deep neural networks as a model for the policy and evaluation in MCTS.
This quality of this model could be enhanced by additional data augmentation by
training and testing existing and synthesized tactics.
Conjecturing suitable intermediate steps could also allow \tactictoe to solve
a problem which requires a long proof by decomposing it into multiple easy
steps.

\paragraph{Acknowledgments}\label{sect:acks}
We would like to thank Lasse Blaauwbroek for his insightful comments which
contributed to improve the quality of this paper. This work has been supported
by the ERC Consolidator grant no.\ 649043 \textit{AI4REASON} and ERC starting
grant no.\ 714034 \textit{SMART}.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
% LocalWords: TacticToe tactictoe precedences ITP ITPs THENL
